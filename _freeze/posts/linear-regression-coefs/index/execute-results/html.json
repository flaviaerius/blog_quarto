{
  "hash": "641c5ecff5df7f4b14ec27ded9de2418",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Understand Linear Regression Results in R\"\nauthor: \"Flávia E. Rius\"\ndate: \"2023-07-20\"\ncategories: [data science, R, statistics, machine learning, supervised learning, regression]\nwebsite:\n  google-analytics: \"G-S78JRQSZER\"\n---\n\n\n\n\n\n\nIn this post I will write about the statistics and metrics of linear regression for you to finally understand all of the output from `summary()` R function applied to an `lm()` object, in a very practical way.\n\n__Disclaimer__: I am __NOT__ a statistician, so to all statisticians out there, I apologize for any misused term. My intention here is just to let people interpret their own regression models and __understand__ what is going on with their data in a practical way.\n \n## About linear regression\n\nThere are two main uses of linear regression: statistical inference and prediction.\n\nIn the statistical inference line, a __linear regression__ is performed to find a relationship of a dependent continuous variable and one or more variables of any type (continuous or categorical). You can, then, __explain__ a relationship between variables.\n\nAs a predictive point of view, regression is a part of __supervised learning__, a type of __machine learning__, and can be used to predict new data based on the relationship found between two variables.\n\nThese two approaches are not mutually exclusive, they can be used together. You can both explain the relationship between variables and use your model to predict with new data input.\n\nFor more insights on this difference, I recommend [this brief Nature discussion and example on RNA-seq data](https://www.nature.com/articles/nmeth.4642.pdf?ssp=1&darkschemeovr=1&setlang=pt-BR&safesearch=moderate), especially the first four paragraphs. \n\nConcerning the prior assumptions to perform a linear model, such as __linearity__ and __normality__ of the variables, mainly, this is a very discussed topic among statisticians and data scientists. In the __machine learning__ side, the assumptions do not matter that much if the model has a good application in predicting the variable of interest. And for the inference side, the assumptions are more important for the model inference to be correct, but they are not written in stone. For example, [not normally distributed variables might work well in a t-test or linear regression for a large sample size](https://www.annualreviews.org/doi/pdf/10.1146/annurev.publhealth.23.100901.140546). At the same time, to perform linear regressions on gene expression data from RNA-seq, there are several transformations made in the data so it can be normal, linear, and homoskedastic. \n\nAs I said, it is a very much discussed topic and I suggest that you read more about your area of application to decide if you should check for assumptions or not. I won't dive into assumptions in this post.\n\n## Running the linear regression\n\nFirst of all, we need an analysis. \n\nWe'll use the most classic dataset of R: `mtcars`. It contains measures as weight, miles per gallon, number of cylinders, etc, of 32 models of cars from 1974. You can explore the dataset more by typing `?mtcars` in your R console.\n\nLet's check the variables distribution overall:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(mtcars)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n      mpg             cyl             disp             hp       \n Min.   :10.40   Min.   :4.000   Min.   : 71.1   Min.   : 52.0  \n 1st Qu.:15.43   1st Qu.:4.000   1st Qu.:120.8   1st Qu.: 96.5  \n Median :19.20   Median :6.000   Median :196.3   Median :123.0  \n Mean   :20.09   Mean   :6.188   Mean   :230.7   Mean   :146.7  \n 3rd Qu.:22.80   3rd Qu.:8.000   3rd Qu.:326.0   3rd Qu.:180.0  \n Max.   :33.90   Max.   :8.000   Max.   :472.0   Max.   :335.0  \n      drat             wt             qsec             vs        \n Min.   :2.760   Min.   :1.513   Min.   :14.50   Min.   :0.0000  \n 1st Qu.:3.080   1st Qu.:2.581   1st Qu.:16.89   1st Qu.:0.0000  \n Median :3.695   Median :3.325   Median :17.71   Median :0.0000  \n Mean   :3.597   Mean   :3.217   Mean   :17.85   Mean   :0.4375  \n 3rd Qu.:3.920   3rd Qu.:3.610   3rd Qu.:18.90   3rd Qu.:1.0000  \n Max.   :4.930   Max.   :5.424   Max.   :22.90   Max.   :1.0000  \n       am              gear            carb      \n Min.   :0.0000   Min.   :3.000   Min.   :1.000  \n 1st Qu.:0.0000   1st Qu.:3.000   1st Qu.:2.000  \n Median :0.0000   Median :4.000   Median :2.000  \n Mean   :0.4062   Mean   :3.688   Mean   :2.812  \n 3rd Qu.:1.0000   3rd Qu.:4.000   3rd Qu.:4.000  \n Max.   :1.0000   Max.   :5.000   Max.   :8.000  \n```\n\n\n:::\n:::\n\n\n\n\n&nbsp;\n\nThey are all numeric, but `am` and `vs` seem to be binary, and some seem to be categorical, as `carb` and `cyl`.\n\nIn our analysis to demonstrate the linear regression metrics and statistics, our __question__ will be: \n\n> Does the weight of a car influences in how many miles it can go per gallon of gas?\n\nTo answer that, we will use the predictor (independent variable) __weight__ or `wt` and the predicted (dependent variable) __miles per gallon__ or `mpg`.\n\nLet's fit the model:\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit <- lm(mpg ~ wt, mtcars)\n```\n:::\n\n\n\n\n&nbsp;\n \nTo obtain all statistics and some of the metrics for our model, we need to use the function `summary()` in R.\n\n&nbsp;\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(fit) \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = mpg ~ wt, data = mtcars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.5432 -2.3647 -0.1252  1.4096  6.8727 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  37.2851     1.8776  19.858  < 2e-16 ***\nwt           -5.3445     0.5591  -9.559 1.29e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.046 on 30 degrees of freedom\nMultiple R-squared:  0.7528,\tAdjusted R-squared:  0.7446 \nF-statistic: 91.38 on 1 and 30 DF,  p-value: 1.294e-10\n```\n\n\n:::\n:::\n\n\n\n\n&nbsp;\n\nOkay, so we have this ocean of information and how can we answer our question? p value is low, and all three stars for coefficients, this seems nice.\n\nLet's see one by one, by order of appearance in the output of `summary(fit)`.\n\n## Metrics and statistics\n\nWhile __metrics__ are the values to measure performance of the overall model, __statistics__ are the coefficients of hypothesis tests and estimates of each variable. Below they will be explained.\n\nTo visualize some parts of the explanation, the linear regression plot will be illustrated below.\n\n\n\n\n::: {.cell .preview-image}\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\n\n\n### __1. Residuals__\n\n&nbsp;\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Residuals:\n#     Min      1Q  Median      3Q     Max \n# -4.5432 -2.3647 -0.1252  1.4096  6.8727 \n```\n:::\n\n\n\n\n&nbsp;\n\n\"Residual\" was a term invented because an executive of a drug industry didn't want to admit having \"error\" in their data when sending it to FDA (you can find this info on page 239 of the book [The Lady Tasting Tea](https://www.goodreads.com/book/show/106350.The_Lady_Tasting_Tea?from_search=true&from_srp=true&qid=x3RuL9nMWd&rank=1)). So yes, these are __the \"errors\" of your model__. Not that there is something wrong, but the fitted line is not perfect, there are deviations from it to your real data because other factors and randomness affect your predicted variable (`mpg`) too. Those deviations are the residuals. \n\nThey are calculated subtracting the predicted data from the observed data. In our case, predicted (obtained using the model) miles per gallon minus observed (from `mtcars`) miles per gallon.\n\nTo see all of them, instead of a summary, run:\n\n&nbsp;\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nresid(fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n          Mazda RX4       Mazda RX4 Wag          Datsun 710      Hornet 4 Drive \n         -2.2826106          -0.9197704          -2.0859521           1.2973499 \n  Hornet Sportabout             Valiant          Duster 360           Merc 240D \n         -0.2001440          -0.6932545          -3.9053627           4.1637381 \n           Merc 230            Merc 280           Merc 280C          Merc 450SE \n          2.3499593           0.2998560          -1.1001440           0.8668731 \n         Merc 450SL         Merc 450SLC  Cadillac Fleetwood Lincoln Continental \n         -0.0502472          -1.8830236           1.1733496           2.1032876 \n  Chrysler Imperial            Fiat 128         Honda Civic      Toyota Corolla \n          5.9810744           6.8727113           1.7461954           6.4219792 \n      Toyota Corona    Dodge Challenger         AMC Javelin          Camaro Z28 \n         -2.6110037          -2.9725862          -3.7268663          -3.4623553 \n   Pontiac Firebird           Fiat X1-9       Porsche 914-2        Lotus Europa \n          2.4643670           0.3564263           0.1520430           1.2010593 \n     Ford Pantera L        Ferrari Dino       Maserati Bora          Volvo 142E \n         -4.5431513          -2.7809399          -3.2053627          -1.0274952 \n```\n\n\n:::\n:::\n\n\n\n\n&nbsp;\n\nIt is possible to __explore the residuals__ checking them for some assumptions of a linear regression, and hidden patterns in the data. This is a very long topic, so it will be left to another post.\n\n### __2. Coefficients__\n\nOverall, the coefficients contain information about the predictors of your dependent variable. Below you can see a detailed explanation.\n\n\n&nbsp;\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Coefficients:\n#             Estimate Std. Error t value Pr(>|t|)    \n# (Intercept)  37.2851     1.8776  19.858  < 2e-16 ***\n# wt           -5.3445     0.5591  -9.559 1.29e-10 ***\n```\n:::\n\n\n\n\n&nbsp;\n\n  __a. Intercept__\n  \n  Intercept is the mean value of y when x is 0 in our model. In our example, how many miles a gallon would make in average for a car with __zero lbs__. Also represented as β0 in the linear regression equation: \n  \n  __y = β0 + β1x + ε__\n  \n  In our example it makes no sense at all. There is no car with 0 lbs. Therefore we can just ignore it and interpret the coefficient of interest, β1.\n  \n  __b. wt__\n  \n  Represented by the variable name, this is the angular coefficient, β1. It represents the slope of the line in the linear regression plot. This is how much __y__ increases or decreases with the increase of __one unit of x__. In our example, it is how many __miles per gallon__ _decreases_ (estimate is negative = -5.3) with the _increase_ of each __1000 lb weight__ (unit of measure of wt) of the car.\n  \n  The columns for the coefficients are:\n  \n  __I. Estimate__\n    \n  Estimates are the values per se of β0 (`Intercept`) and β1 (`wt`). The explanation of them is above.\n  \n  __II. Std Error__\n    \n  Standard Error is the deviation of the estimate from its real value. The smallest this value is, more precise the estimate is. It tends to be smaller if you have a big number of observations, since it is the standard deviation divided by n. \n  \n  __III. t value__ \n    \n  This is the value of t in the Student's t test for one sample. The alternative hypothesis tested is: is your estimate different than zero?, which relies on the distribution of your data being a t distribution, which is very close to a normal distribution. It is calculated by Estimate/Std Error.\n  \n  __IV. Pr(>|t|)__\n    \n  This is the __p value__ for the t test applied to your estimate. If it is less than the considered alpha - the error you choose to accept (generally 0.05) - then you can say your estimates are very likely to be significant. \n  \n### __3. Signif. codes__\n\nThis is just a legend for what the asterisks and dot mean concerning level of significance for the p values.\n\n### __4. Degrees of freedom__\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# 30 degrees of freedom\n```\n:::\n\n\n\n\n&nbsp;\n\n__Degrees of freedom__ (__DF__) are how many units of your data are \"free to vary\".\n\nPractically, this is how many observations you have minus the number of estimated parameters used in your model (in this case, intercept and weight). Which means that it is directly proportional to the number of samples, and inversely proportional to the number of parameters and variables in your analysis. Degrees of freedom are crucial to determine your t distribution shape and what will be the significance (p value) of the estimates. \n\n### __5. Residual standard error__\n\n&nbsp;\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Residual standard error: 3.046\n```\n:::\n\n\n\n\n&nbsp;\n\n__Residual standard error__ (__RSE__) is the average deviation of predicted values (from the model) from observed values (the ones in your dataset). \n\nIt can be calculated with:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ny <- mtcars$mpg\ny_hat <- predict(fit)\ndf <- 30 # degrees of freedom\n\nsqrt(sum((y - y_hat)^2)/df)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 3.045882\n```\n\n\n:::\n:::\n\n\n\n\n&nbsp;\n\nStatistically speaking, it is the estimate of standard deviation of predicted values from real values; a measure of variation around the regression line. \n\nConfidence intervals around predicted values are generated using the __RSE__, therefore it is an important metric of your model. Large __RSE__ can generate inaccurate predictions.\n\n### __6. Multiple R-squared__\n\n&nbsp;\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Multiple R-squared:  0.7528\n```\n:::\n\n\n\n\n&nbsp;\n\nThis is the famous R squared (R²). The proportion of variance of your dependent variable, y, which is explained by your model. Simplifying: proportion of variance explained. \n\nSince it is a proportion, its value is between 0 and 1. \n\nOur model explains 75% of variance in miles per gallon.\n\nThis does not mean that if you have an R² = 0.3 for example you have a poor regression analysis. Some variables are just partly explained by the predictor analyzed, and if your intention is to interpret the influence of a predictor in a response variable, it may be useful even with a low variance explained. This happens in genomics, with polygenic scores for example, when your analyzed genetics explains only a part of a trait or disease, while the rest is explained by other factors such as environment.\n\nAlso, this is not the best way to analyze if your model is good or not. `RMSE` (Root Mean Squared Error), which is just squaring residuals, averaging them, and getting the square root, is generally a better way to evaluate whether you have a less prone to errors model. The smallest the RMSE, the better your model is. \n\n&nbsp;\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nRMSE <- sqrt(mean(resid(fit)^2))\n\nRMSE\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 2.949163\n```\n\n\n:::\n:::\n\n\n\n\n&nbsp;\n\nThat means that in average there is an error of 2.9 miles per gallon in our model.\n\nHave in mind what my boss always repeats: \"All models are wrong, but some are useful\", a classical phrase by George Box.\n\n### __7. Adjusted R-Squared__\n\n&nbsp;\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Adjusted R-squared:  0.7446\n```\n:::\n\n\n\n\n&nbsp;\n\nWhen there is more than one predictor in your linear regression (multiple linear regression), there is always an increase in R² independent of increase in variance explained, due to just the addittion of a new predictor. Therefore the R² value is adjusted for that.  \n\nTo compare multiple linear regression models, or models with different number of predictors, it is recommended to check adjusted R-squared instead of multiple R-squared for variance explained.\n\n### __8. F-statistic__\n\n&nbsp;\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# F-statistic: 91.38 on 1 and 30 DF\n```\n:::\n\n\n\n\n&nbsp;\n\nF is the statistic from the F-test performed in your model. Basically the F value is used to see if there is any relationship between the response and predictors in a __multiple linear regression__. In the example discussed here, a simple linear regression, the estimate β1 = 0 is a better way to tell that there is no relationship between the response and predictor.\n\n### __9. p-value__\n\n&nbsp;\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# p-value: 1.294e-10\n```\n:::\n\n\n\n\n&nbsp;\n\nThis is the F-test p-value, which tells you if your F-test is significant. A p-value below the alpha you choose (for example, 0.05) means there is a high chance that at least one of your predictors is significantly associated with your dependent variable. It is more used for multiple regression models, which can be approached in another blog post.\n\n\n## Interpretation concerns\n\nSome transformations can be used in the linear regression to make your model more interpretable.\n\nFor example, let's say that, as me, you are not from a country that uses _lbs_ as a weight metric, neither _miles_ for a distance one, or _gallons_ for a volume one. \n\nInstead, you would like to interpret your model using _kilograms_, _kilometers_, and _liters_. Is this possible?\n\n__Yes!__\n\nUsing the `I()` operator around the terms of the linear regression, just so you can transform from inside of the `lm()` function, we can transform the terms and get the proper interpretation. See below:\n\n&nbsp;\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmiles_to_kilometers <- 1.61\ngallons_to_liters <- 3.79\nlb_to_kg <- 0.45\n\nfit_not_english <- lm(I(mpg*miles_to_kilometers/gallons_to_liters) ~ I(wt*lb_to_kg), mtcars)\n\nsummary(fit_not_english)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = I(mpg * miles_to_kilometers/gallons_to_liters) ~ \n    I(wt * lb_to_kg), data = mtcars)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.92994 -1.00453 -0.05318  0.59878  2.91954 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(>|t|)    \n(Intercept)       15.8388     0.7976  19.858  < 2e-16 ***\nI(wt * lb_to_kg)  -5.0452     0.5278  -9.559 1.29e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.294 on 30 degrees of freedom\nMultiple R-squared:  0.7528,\tAdjusted R-squared:  0.7446 \nF-statistic: 91.38 on 1 and 30 DF,  p-value: 1.294e-10\n```\n\n\n:::\n:::\n\n\n\n\n&nbsp;\n\nThe interpretation is:\n\n- There is a reduction of 5 kilometers per liter for each 1000kg increase in the car weight.\n\nMuch better now, right?\n\n;) \n\n## Closing remarks\n\nI hope you have liked this post, and that it has shed light on you about how to interpret the regression models outputs from R. \n\nI really missed this content when I was applying linear regression in R, so I really hope it helps you as much as it would have helped me when I was looking for it!\n\n## Read more\n\n- Chapter 3.2 of the book [An Introduction to Statistical Learning (Second Edition)](https://www.statlearning.com/) - you can pick the R or Python version to download __for free__. \n\n- __Regression Models Coursera Course__ from Johns Hopkins [__free GitHub content__](https://github.com/bcaffo/regmodsbook).\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}