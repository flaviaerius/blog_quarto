---
title: 'How to operate large files in bioinformatics'
author: 'Flávia E. Rius'
date: '2025-03-25'
categories: [bioinformatics, cloud computing]
website:
    google-analytics: 'G-S78JRQSZER'
---

Situation: you start very happy that you will be analyzing your sequencing data, be it transcriptomic, genomic or epigenomic; you have learned bash, R and statistics, and then you try to transfer all of the data to your computer, and then you see that, bleh, you don't have enough storage on your computer. Then what?

You start panicking? No!

You start searching the web for any information on that, and come up with a gazillion new terms that you have never heard before. _Server_, _cluster_, _cloud computing_, _ssh_, _ssd_, _bucket_, to enumerate a few. And now you panic for real.

![](DALL·E 2025-02-18 21.49.29 - A beginner programmer sitting nervously in front of a large computer screen filled with complex lines of code. The person, wearing casual clothes, has.webp){fig-alt="A beginner bioinformatician worried about operating large files" width=50%}.

Don't worry, I am here to help! I'll get straight to the point about the options available to tackle this problem when working with big data in bioinformatics.

Basically you have two main options:

- Cloud computing

- Server/cluster of your institution (when available)

## Cloud computing

These are the services provided by companies in which you can pay for a temporary remote machine to run your analyses. Google Cloud, Amazon AWS and Azure are the most well-known providers of this type of service. Basically you need to create an account, link to a payment method, and start using it.

If this is your option, you need to learn the basics of analyzing data in the cloud. I would go with the simplest way, which is creating an instance. An instance is nothing more than a computer you configure remotely. You need to pick a size of CPU, RAM memory and storage, which can be HDD (popularly known as HD, cheaper and slower) or SSD (faster to transfer data, but more expensive). Then, you just transfer your data to the instance using wget or ssh, depending on where this data will come from, and you can work from there (using the command line, of course).

### Google Cloud Compute Engine 


Below I will give you some details on how to do this on Google Cloud Compute Engine.
They give $300 of free credits to new Google Cloud users, so you can test and potentially do the analysis you need for free. To give you an idea of costs, a general use machine with 4 CPUs, 15GB of memory (RAM) and 100GB of HDD storage,  running for 30 days [would cost you 140 USD](https://shorturl.at/I9eY2). You can calculate the expected amount for your personalized needs in the calculator page.

After setting up your instance and transferring the data, you need to install all software needed to run your analyses from scratch. For example, let's say that you want to run an RNA-seq alignment using STAR. You will need to install STAR, transfer the reference genome or transcriptome to the instance, build the reference for this program, and then run it. Don't forget that any other tools you might need for previous steps such as QC must be installed to the instance too. After all, **it is a new computer you are renting for a limited time**.

After finishing your analysis, you need to deal with the resulting data. Let's talk about this in another post, because it is a topic that deserves better attention.

## Server/cluster in your university
![Photo by <a href="https://unsplash.com/@tvick?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash">Taylor Vick</a> on <a href="https://unsplash.com/photos/cable-network-M5tzZtFCOfs?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash">Unsplash</a>](server-unsplash.jpg){width=70%}

In this option there are two different ways that a server might be provided to you: via a job schedule strategy or by giving you access to a slice of this server.

A server, as the instance in the cloud computing resource, is a remote computer to which you have access. The job schedule strategy requires you to learn how to structure your data and then you need to write your scripts and submit them to the job scheduler, then wait for your turn to run them, as other people send their jobs to the scheduler too. The most popular examples are Slurm and SGE.

Generally there is a free limited space (scratch) of the server for you to experiment with a slice of the data (so you are certain that your script is correct to send to schedule for example). Some more complex labs allow you to send to a specific server schedule line, depending on your demand of cpu and processing. These are aspects you may require by task as well when submitting your job to the scheduler.

Another way of doing this, generally in less experienced labs, is by requesting a specific slice of the server available in the university for you to use during your full project. In my PhD I knew that I could have asked for it in my university, but I did not have a bit of idea on what would be my CPU, RAM memory and storage needs. Therefore I could not get it. I still relied on my collaborator server to finish my analyses. So this is an important knowledge to have.

As important as it might sound, calculating the necessary computational resources for your bioinformatics analyses will need to be addressed in a new blog post. It depends on too many factors.

I hope this post has helped you on your bioinformatics journey!

Did I miss any important points? Let me know in the comments below if you have something to add to this post!
