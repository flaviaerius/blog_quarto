[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hi!\nI am Flávia!\nI am a data scientist with a background on biomedical sciences. I have switched from wet lab to dry lab during my PhD, in which I studied cancer epigenomics, using a lot of bioinformatics both in R and bash.\nNow I work as a data scientist in Mendelics, the biggest genomics laboratory of Latin America. Mostly I work with research and analysis of DNA sequencing data.\nI also mentor people in R programming, statistics and bioinformatics.\nI have 118 5-star reviews in codementor. I really love to have this contact with people all over the world, and help them with their struggle coding or understanding a concept, especially because I’ve been there before and I know how hard that is.\nI love learning new things!\nI wanted to learn everything about everything! But I know I can’t, so I focus on some few topics at a time, being now mostly data science and genomics-related, apart from other variable topic.\nI believe mens sana in corpore sano is totally true, so I like doing some exercise, and I have ran a couple of half marathons before - not suited for any ball-involving sport though.\nWelcome! I hope you enjoy and that you can take something useful or entertaining from here :)\nFeel free to send me an email or connect on LinkedIn with the links below."
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Flávia E. Rius",
    "section": "",
    "text": "How To Operate Large Files in Bioinformatics\n\n\n\n\n\n\nbioinformatics\n\n\ncloud computing\n\n\n\n\n\n\n\n\n\nMar 25, 2025\n\n\nFlávia E. Rius\n\n\n\n\n\n\n\n\n\n\n\n\nUnderstand Linear Regression Results in R\n\n\n\n\n\n\ndata science\n\n\nR\n\n\nstatistics\n\n\nmachine learning\n\n\nsupervised learning\n\n\nregression\n\n\n\n\n\n\n\n\n\nJul 20, 2023\n\n\nFlávia E. Rius\n\n\n\n\n\n\n\n\n\n\n\n\n7 tips for Academic Researchers on Their Way To Data Science\n\n\n\n\n\n\ndata science\n\n\nR\n\n\nPhD\n\n\nacademia\n\n\n\n\n\n\n\n\n\nAug 10, 2022\n\n\nFlávia E. Rius\n\n\n\n\n\n\n\n\n\n\n\n\nStuck in an Error Message? Let Me Help You Fix It!\n\n\n\n\n\n\ndata science\n\n\nR\n\n\nerror\n\n\n\n\n\n\n\n\n\nJun 14, 2022\n\n\nFlávia E. Rius\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/seven-tips-academia-to-data-science/index.html",
    "href": "posts/seven-tips-academia-to-data-science/index.html",
    "title": "7 tips for Academic Researchers on Their Way To Data Science",
    "section": "",
    "text": "Very often people ask me about my transition from academic research to a position as a data scientist. I have compiled in this article some advice to researchers in academia (masters degree, PhD, postdoctoral research) who want to be data scientists in the technology industry1.\nMy focus of learning was in the genomics area, but the tips are useful for other areas too. When I got into my PhD program, I didn’t know the career of data scientist, so I focused in learning bioinformatics. The division between those two sciences in the genomics area is very tenuous. The bioinformatician has a bigger focus on the computational part, developing sequencing data analysis pipelines, software, and orchestrating the interface with data in the cloud. On the other hand, the data scientist is more focused in analyzing research and development (R&D) data, building models, and in delivering information in the format of tables and graphs, which are necessary for development of a new product, or evaluation of a project. Both do a little bit of each other’s function, but basically, the distinction between them is the one I have explained.\nBelow I will give you seven tips I have joined together based on my learning process and career transition from bench scientist (wet lab) to 100% computational researcher (dry lab), and, afterwards, to a position as a data scientist at Mendelics, the biggest company focused in genomic analysis of Latin America."
  },
  {
    "objectID": "posts/seven-tips-academia-to-data-science/index.html#dont-give-up-on-learning-programming",
    "href": "posts/seven-tips-academia-to-data-science/index.html#dont-give-up-on-learning-programming",
    "title": "7 tips for Academic Researchers on Their Way To Data Science",
    "section": "1. Don’t give up on learning programming",
    "text": "1. Don’t give up on learning programming\nI’ve had SO much difficulty to learn R, my first scripts were awful, and I barely understood what I was doing. I started by only replicating tutorials, and, as the time passed, I tried to understand what the functions were doing, and the different rules of R. A small notebook was where I kept my notes about every new function I discovered. One colleague said to me, in an introduction to R course: “The learning curve is very slow in the beginning, but afterwards you get into the plateau phase, and it turns up very easy!”. She was right. Not that I know everything, but today is much easier to learn something new, now that I already know how R works basically."
  },
  {
    "objectID": "posts/seven-tips-academia-to-data-science/index.html#know-a-bit-about-computing",
    "href": "posts/seven-tips-academia-to-data-science/index.html#know-a-bit-about-computing",
    "title": "7 tips for Academic Researchers on Their Way To Data Science",
    "section": "2. Know a bit about computing",
    "text": "2. Know a bit about computing\nTo differentiate RAM memory from disk memory, and what is a processor are parts of the first steps for you to have an idea about computing. You will also need to have a notion of your machine’s capacity, and of the disk space that a file occupies, to architect about a way to manipulate it. Besides that, to know that the server is nothing more than another computer connected with yours by a “bridge” via ssh command, just as the access to cloud computing or storage. Nothing exists online-only. Several computers and disk units (HD or SSD) are present in fixed places and connected to your computer by “bridges”.\nI have used the online course Introduction to Computer Science, from Wikiversity, to learn the basic concepts of computation in the beginning. To understand what is behind that black screen and of the commands will clear your mind tangled about so many unknown novelties."
  },
  {
    "objectID": "posts/seven-tips-academia-to-data-science/index.html#study-statistics",
    "href": "posts/seven-tips-academia-to-data-science/index.html#study-statistics",
    "title": "7 tips for Academic Researchers on Their Way To Data Science",
    "section": "3. Study statistics",
    "text": "3. Study statistics\nHere, my advice is to study as the statistical tests appear in your project. Wanting to know absolutely everything about statistics won’t help, as it is such a vast area.\nInstead of that, try to understand the reason of each and every statistical test of your work. What question is the test trying to answer exactly? It seems like I am telling a trivial thing, that everybody knows, after all, the project belongs to the researcher working on it, but very often you are just copying what others have done, and do not understand what is happening there. This has happened with me sometimes. For example, the survival analysis I have used to understand the influence of some covariables in survival of melanoma patients was applied with a very shallow knowledge of mine in the beginning. But I went after understanding better about Hazard Ratio interpretation, and robustness of Cox regression (multivariate) compared to Kaplan-Meier (univariate), and, with that, acquired more knowledge about this statistical analysis. And, of course, I am still learning each day more about this fascinating area that is statistics."
  },
  {
    "objectID": "posts/seven-tips-academia-to-data-science/index.html#dont-try-to-complicate-the-simple",
    "href": "posts/seven-tips-academia-to-data-science/index.html#dont-try-to-complicate-the-simple",
    "title": "7 tips for Academic Researchers on Their Way To Data Science",
    "section": "4. Don’t try to complicate the simple",
    "text": "4. Don’t try to complicate the simple\nIt is like when you are learning a new language. If you are learning Spanish, you don’t need to use difficult words to speak, go with the simple, the vocabulary that you already know. With scripts it works the same way.\nWrite the code as simple as possible, and aim at learning more complex functions and tools, such as for loops, lapply in case of R language, or if clauses. Of course you will use a lot of ctrl+c and ctrl+v on stackoverflow code that you have no idea on how it works, but that solves your problem. With time, you will start to search, read the documentation of that function, test it in other datasets, take out a variable, add another one, and learn what the function does. I recommend to include a to-do list with the abilities you want to acquire in R. I did that with the for loop. See the new function as a new word in a vocabulary, and the for loop structure as a new syntax. While you still don’t know how to shorten your lines of code, there is no problem in copying and pasting several times. If it works, go ahead!"
  },
  {
    "objectID": "posts/seven-tips-academia-to-data-science/index.html#always-explore-the-data-you-are-analyzing",
    "href": "posts/seven-tips-academia-to-data-science/index.html#always-explore-the-data-you-are-analyzing",
    "title": "7 tips for Academic Researchers on Their Way To Data Science",
    "section": "5. Always explore the data you are analyzing",
    "text": "5. Always explore the data you are analyzing\nThis is a basic one, but I also didn’t use in the beginning. I used to analyze stuff by repeating tutorials and never taking a look at the start and intermediate data. If you use head command, see the columns your dataset has, number of lines, and if variables are numeric or not, minimum, maximum, and mean values, you have a better idea on what is happening in that tutorial you are replicating. Do it not only with the initial dataset, but also with the intermediate ones, and with the final. This way you will learn what is happening in each step of your data processing. Likewise observe new variables created, and the output of some functions in the console."
  },
  {
    "objectID": "posts/seven-tips-academia-to-data-science/index.html#learn-to-google-the-error-message",
    "href": "posts/seven-tips-academia-to-data-science/index.html#learn-to-google-the-error-message",
    "title": "7 tips for Academic Researchers on Their Way To Data Science",
    "section": "6. Learn to google the error message",
    "text": "6. Learn to google the error message\nAnd also to search what you need to do with your code. As the time passes you learn the best way to search, and how to write the query to find what you want. I believe this is related to acquiring more vocabulary in the programming language you are dealing with. The error messages are unavoidable, and, unless they give you a clue that allow the solution to the problem (eg. class is numeric and you need a factor, in R), you will need to google them. Then you probably will run into an answer from stackoverflow, from a github issue, or even from a blog post about that type of error message, and how to fix it. Before publishing a question in stackoverflow, TURN THE INTERNET UPSIDE DOWN to know if someone hasn’t asked that before. Chances are 99% that the answer is already somewhere, especially when you are a beginner. If there is no such question, then it is OK for you to publish it."
  },
  {
    "objectID": "posts/seven-tips-academia-to-data-science/index.html#organize-your-data",
    "href": "posts/seven-tips-academia-to-data-science/index.html#organize-your-data",
    "title": "7 tips for Academic Researchers on Their Way To Data Science",
    "section": "7. Organize your data",
    "text": "7. Organize your data\nIt is important to have a structure that you understand, because a research needs reproducibility, and you inevitably will need to go back and run that script again, to make a small adjustment in the plots before submitting your manuscript to the journal, for example. So it is necessary that a logical structure of folders (directories) exists, and a documentation about where to find each important data of your research. This skill will be highly transfered to your data science job.\nAlthough it will be a lot of work, it is very important to have documentation. The organization may not be trivial in the beginning, but it is worth making an effort to structure your scripts and data in a certain moment. Your self from the future will be definitely very grateful.\nThese are the tips I wanted to pass along. Undoubtedly I would have benefited if I have read them while I was learning, so I believe they may have a value to you, if you are learning as well. Certainly the abilities I have acquired following those tips are fundamental to what I do nowadays as a data scientist at Mendelics, and also as a mentor. Then, if you want to follow this path, and, as me, doesn’t have a background in computation or statistics, enjoy, learn with my mistakes, and go on to this world of data science.\nIf you have any questions, want to know more about any of these topics, or even if you have a suggestion, feel free to contact me; I will be glad to hear from you.\nAll the best,\nFlávia E. Rius"
  },
  {
    "objectID": "posts/seven-tips-academia-to-data-science/index.html#footnotes",
    "href": "posts/seven-tips-academia-to-data-science/index.html#footnotes",
    "title": "7 tips for Academic Researchers on Their Way To Data Science",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nArticle originally published in Portuguese at Linkedin↩︎"
  },
  {
    "objectID": "posts/tips-fix-error/index.html",
    "href": "posts/tips-fix-error/index.html",
    "title": "Stuck in an Error Message? Let Me Help You Fix It!",
    "section": "",
    "text": "This is for R beginners, or even intermediate and advanced users who want a different perspective about how to debug errors in R.\nAlways Sometimes you face errors in R, it is unavoidable. I will give you three different tools to debug the errors on your own, and avoid getting stuck not knowing what to do.\n\n1. Look for typos in your code.\nAs simple as that. Very often they are the origin of the error message. A parentheses missing, or in the wrong place, same as a comma, can change R interpretation and lead to different errors. Same if you add the wrong variable name.\n\n# For example, forgetting a {\n\nfor(y in 1:10) {\n if(y &gt; 2) {\n   print(paste(y, \"is bigger than 2.\"))\n }\n  else {\n    print(paste(y, \"is not bigger than 2.\"))\n  #### It should be here ####\n}\n\nError in parse(text = input): &lt;text&gt;:12:0: unexpected end of input\n10: }\n11: \n   ^\n\n\n\n\n2. Use the great question mark tool in R to access what the function does.\nBy typing ?[function], you will be able to see a help page, very often well described. Such as:\n\n?mean\n\n\n\nR: Arithmetic Mean\n\n\n\n\n\n\n\n\n\n\n\n\nmean\nR Documentation\n\n\n\n\n\nArithmetic Mean\n\nDescription\n\nGeneric function for the (trimmed) arithmetic mean.\n\n\n\nUsage\n\nmean(x, ...)\n\n## Default S3 method:\nmean(x, trim = 0, na.rm = FALSE, ...)\n\n\n\nArguments\n\n\n\n\n\nx\nan R object. Currently there are methods for numeric/logical vectors and date, date-time and time interval objects. Complex vectors are allowed for trim = 0, only.\n\n\ntrim\nthe fraction (0 to 0.5) of observations to be trimmed from each end of x before the mean is computed. Values of trim outside that range are taken as the nearest endpoint.\n\n\nna.rm\na logical evaluating to TRUE or FALSE indicating whether NA values should be stripped before the computation proceeds.\n\n\n...\nfurther arguments passed to or from other methods.\n\n\n\n\n\n\nValue\n\nIf trim is zero (the default), the arithmetic mean of the\nvalues in x is computed, as a numeric or complex vector of\nlength one.  If x is not logical (coerced to numeric), numeric\n(including integer) or complex, NA_real_ is returned, with a warning.\n\nIf trim is non-zero, a symmetrically trimmed mean is computed\nwith a fraction of trim observations deleted from each end\nbefore the mean is computed.\n\n\n\nReferences\n\nBecker, R. A., Chambers, J. M. and Wilks, A. R. (1988)\nThe New S Language.\nWadsworth & Brooks/Cole.\n\n\n\nSee Also\n\nweighted.mean, mean.POSIXct,\ncolMeans for row and column means.\n\n\n\nExamples\n\nx &lt;- c(0:10, 50)\nxm &lt;- mean(x)\nc(xm, mean(x, trim = 0.10))\n\n\n\n\n\n\n\n\nWhen you get errors of type: you are inserting a data frame when it should be a numeric vector, for example, you will be able to find this out by reading each parameter of a function in this help document. And pay attention to the specification of class of the vector you should add to each argument of your function. In our example here, x should be an R object of classes numeric, logical, date, date-time or time interval.\nAlso, looking at the examples could be the best way to figure out how a function works. Testing this example and exploring the output is a great way to figure out which class or shape you need for the function too, because sometimes this is not so well documented. You may need unique values or a numeric instead of factor (this happens often for statistical functions).\nUse the trial and error technique.\nApply what you have read and what you think you have understood by the help page in your code to see if that is really it. Even to a smaller piece of your dataset, so it don’t take too long to run. This is very important for understanding better the functions and reach what you want. See the output of the example and compare it to the one using your data.\n\n\n3. Google the error message.\nIf none of the alternatives above works, just go and Google it.\nYou will find answers in stackoverflow, GitHub issues, and blog posts, R forums, among other vehicles of information. Read all of the answers, not just the first one, and the comments too. This increases the chance for you to find that piece of information you need.\nSometimes you need to install an additional package or extension to your OS in order to fix the error. And that is fine.\nAlmost always someone has already encounter the same error as you, and the coding community is so awesome that they help anyone with a doubt, and this stays fixed in the forums.\nIf you don’t find your question or any similar in the forum, ask it yourself. In stackoverflow, don’t forget to read the code of conduct and post a reproducible example so the community can help you better. This allows other people to reproduce your error locally and figure out how to fix it.\n\n\nBonus!\nIf you’re feeling lazy, skip the first two steps and Google the error message right away.\nYou may find the answer easily, or find a long blog post with details hard to understand when you are a beginner.\nWhen the documentation for help in R is lacking information, this is better, but to know that you would need to meet the previous step which is looking for the R help message.\nYou are free to decide which path to follow.\nI hope these tips help you in your R journey.\nFeel free to contact me if you find that useful, I will love to hear from you!\nBest of luck!\nFlávia"
  },
  {
    "objectID": "posts/tips-fix-error/index.html#mean",
    "href": "posts/tips-fix-error/index.html#mean",
    "title": "Stuck in an error message? Let me help you fix it!",
    "section": "Arithmetic Mean",
    "text": "Arithmetic Mean\n\nDescription\nGeneric function for the (trimmed) arithmetic mean.\n\n\nUsage\nmean(x, ...)\n\n## Default S3 method:\nmean(x, trim = 0, na.rm = FALSE, ...)\n\n\nArguments\n\n\n\nx\nAn R object. Currently there are methods for numeric/logical vectors and date, date-time and time interval objects. Complex vectors are allowed for trim = 0, only.\n\n\ntrim\nthe fraction (0 to 0.5) of observations to be trimmed from each end of x before the mean is computed. Values of trim outside that range are taken as the nearest endpoint.\n\n\nna.rm\na logical evaluating to TRUE or FALSE indicating whether NA values should be stripped before the computation proceeds.\n\n\n...\nfurther arguments passed to or from other methods.\n\n\n\n\n\n\nValue\n\nIf trim is zero (the default), the arithmetic mean of the\nvalues in x is computed, as a numeric or complex vector of\nlength one.  If x is not logical (coerced to numeric), numeric\n(including integer) or complex, NA_real_ is returned, with a warning.\n\nIf trim is non-zero, a symmetrically trimmed mean is computed\nwith a fraction of trim observations deleted from each end\nbefore the mean is computed.\n\n\n\nReferences\n\nBecker, R. A., Chambers, J. M. and Wilks, A. R. (1988)\nThe New S Language.\nWadsworth & Brooks/Cole.\n\n\n\nSee Also\n\nweighted.mean, mean.POSIXct,\ncolMeans for row and column means.\n\n\n\nExamples\n\nx &lt;- c(0:10, 50)\nxm &lt;- mean(x)\nc(xm, mean(x, trim = 0.10))"
  },
  {
    "objectID": "posts/linear-regression-coefs/index.html",
    "href": "posts/linear-regression-coefs/index.html",
    "title": "Understand Linear Regression Results in R",
    "section": "",
    "text": "In this post I will write about the statistics and metrics of linear regression for you to finally understand all of the output from summary() R function applied to an lm() object, in a very practical way.\nDisclaimer: I am NOT a statistician, so to all statisticians out there, I apologize for any misused term. My intention here is just to let people interpret their own regression models and understand what is going on with their data in a practical way."
  },
  {
    "objectID": "posts/linear-regression-coefs/index.html#about-linear-regression",
    "href": "posts/linear-regression-coefs/index.html#about-linear-regression",
    "title": "Understand Linear Regression Results in R",
    "section": "About linear regression",
    "text": "About linear regression\nThere are two main uses of linear regression: statistical inference and prediction.\nIn the statistical inference line, a linear regression is performed to find a relationship of a dependent continuous variable and one or more variables of any type (continuous or categorical). You can, then, explain a relationship between variables.\nAs a predictive point of view, regression is a part of supervised learning, a type of machine learning, and can be used to predict new data based on the relationship found between two variables.\nThese two approaches are not mutually exclusive, they can be used together. You can both explain the relationship between variables and use your model to predict with new data input.\nFor more insights on this difference, I recommend this brief Nature discussion and example on RNA-seq data, especially the first four paragraphs.\nConcerning the prior assumptions to perform a linear model, such as linearity and normality of the variables, mainly, this is a very discussed topic among statisticians and data scientists. In the machine learning side, the assumptions do not matter that much if the model has a good application in predicting the variable of interest. And for the inference side, the assumptions are more important for the model inference to be correct, but they are not written in stone. For example, not normally distributed variables might work well in a t-test or linear regression for a large sample size. At the same time, to perform linear regressions on gene expression data from RNA-seq, there are several transformations made in the data so it can be normal, linear, and homoskedastic.\nAs I said, it is a very much discussed topic and I suggest that you read more about your area of application to decide if you should check for assumptions or not. I won’t dive into assumptions in this post."
  },
  {
    "objectID": "posts/linear-regression-coefs/index.html#running-the-linear-regression",
    "href": "posts/linear-regression-coefs/index.html#running-the-linear-regression",
    "title": "Understand Linear Regression Results in R",
    "section": "Running the linear regression",
    "text": "Running the linear regression\nFirst of all, we need an analysis.\nWe’ll use the most classic dataset of R: mtcars. It contains measures as weight, miles per gallon, number of cylinders, etc, of 32 models of cars from 1974. You can explore the dataset more by typing ?mtcars in your R console.\nLet’s check the variables distribution overall:\n\nsummary(mtcars)\n\n      mpg             cyl             disp             hp       \n Min.   :10.40   Min.   :4.000   Min.   : 71.1   Min.   : 52.0  \n 1st Qu.:15.43   1st Qu.:4.000   1st Qu.:120.8   1st Qu.: 96.5  \n Median :19.20   Median :6.000   Median :196.3   Median :123.0  \n Mean   :20.09   Mean   :6.188   Mean   :230.7   Mean   :146.7  \n 3rd Qu.:22.80   3rd Qu.:8.000   3rd Qu.:326.0   3rd Qu.:180.0  \n Max.   :33.90   Max.   :8.000   Max.   :472.0   Max.   :335.0  \n      drat             wt             qsec             vs        \n Min.   :2.760   Min.   :1.513   Min.   :14.50   Min.   :0.0000  \n 1st Qu.:3.080   1st Qu.:2.581   1st Qu.:16.89   1st Qu.:0.0000  \n Median :3.695   Median :3.325   Median :17.71   Median :0.0000  \n Mean   :3.597   Mean   :3.217   Mean   :17.85   Mean   :0.4375  \n 3rd Qu.:3.920   3rd Qu.:3.610   3rd Qu.:18.90   3rd Qu.:1.0000  \n Max.   :4.930   Max.   :5.424   Max.   :22.90   Max.   :1.0000  \n       am              gear            carb      \n Min.   :0.0000   Min.   :3.000   Min.   :1.000  \n 1st Qu.:0.0000   1st Qu.:3.000   1st Qu.:2.000  \n Median :0.0000   Median :4.000   Median :2.000  \n Mean   :0.4062   Mean   :3.688   Mean   :2.812  \n 3rd Qu.:1.0000   3rd Qu.:4.000   3rd Qu.:4.000  \n Max.   :1.0000   Max.   :5.000   Max.   :8.000  \n\n\n \nThey are all numeric, but am and vs seem to be binary, and some seem to be categorical, as carb and cyl.\nIn our analysis to demonstrate the linear regression metrics and statistics, our question will be:\n\nDoes the weight of a car influences in how many miles it can go per gallon of gas?\n\nTo answer that, we will use the predictor (independent variable) weight or wt and the predicted (dependent variable) miles per gallon or mpg.\nLet’s fit the model:\n\nfit &lt;- lm(mpg ~ wt, mtcars)\n\n \nTo obtain all statistics and some of the metrics for our model, we need to use the function summary() in R.\n \n\nsummary(fit) \n\n\nCall:\nlm(formula = mpg ~ wt, data = mtcars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.5432 -2.3647 -0.1252  1.4096  6.8727 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  37.2851     1.8776  19.858  &lt; 2e-16 ***\nwt           -5.3445     0.5591  -9.559 1.29e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.046 on 30 degrees of freedom\nMultiple R-squared:  0.7528,    Adjusted R-squared:  0.7446 \nF-statistic: 91.38 on 1 and 30 DF,  p-value: 1.294e-10\n\n\n \nOkay, so we have this ocean of information and how can we answer our question? p value is low, and all three stars for coefficients, this seems nice.\nLet’s see one by one, by order of appearance in the output of summary(fit)."
  },
  {
    "objectID": "posts/linear-regression-coefs/index.html#metrics-and-statistics",
    "href": "posts/linear-regression-coefs/index.html#metrics-and-statistics",
    "title": "Understand Linear Regression Results in R",
    "section": "Metrics and statistics",
    "text": "Metrics and statistics\nWhile metrics are the values to measure performance of the overall model, statistics are the coefficients of hypothesis tests and estimates of each variable. Below they will be explained.\nTo visualize some parts of the explanation, the linear regression plot will be illustrated below.\n\n\n\n\n\n\n\n\n\n\n1. Residuals\n \n\n# Residuals:\n#     Min      1Q  Median      3Q     Max \n# -4.5432 -2.3647 -0.1252  1.4096  6.8727 \n\n \n“Residual” was a term invented because an executive of a drug industry didn’t want to admit having “error” in their data when sending it to FDA (you can find this info on page 239 of the book The Lady Tasting Tea). So yes, these are the “errors” of your model. Not that there is something wrong, but the fitted line is not perfect, there are deviations from it to your real data because other factors and randomness affect your predicted variable (mpg) too. Those deviations are the residuals.\nThey are calculated subtracting the predicted data from the observed data. In our case, predicted (obtained using the model) miles per gallon minus observed (from mtcars) miles per gallon.\nTo see all of them, instead of a summary, run:\n \n\nresid(fit)\n\n          Mazda RX4       Mazda RX4 Wag          Datsun 710      Hornet 4 Drive \n         -2.2826106          -0.9197704          -2.0859521           1.2973499 \n  Hornet Sportabout             Valiant          Duster 360           Merc 240D \n         -0.2001440          -0.6932545          -3.9053627           4.1637381 \n           Merc 230            Merc 280           Merc 280C          Merc 450SE \n          2.3499593           0.2998560          -1.1001440           0.8668731 \n         Merc 450SL         Merc 450SLC  Cadillac Fleetwood Lincoln Continental \n         -0.0502472          -1.8830236           1.1733496           2.1032876 \n  Chrysler Imperial            Fiat 128         Honda Civic      Toyota Corolla \n          5.9810744           6.8727113           1.7461954           6.4219792 \n      Toyota Corona    Dodge Challenger         AMC Javelin          Camaro Z28 \n         -2.6110037          -2.9725862          -3.7268663          -3.4623553 \n   Pontiac Firebird           Fiat X1-9       Porsche 914-2        Lotus Europa \n          2.4643670           0.3564263           0.1520430           1.2010593 \n     Ford Pantera L        Ferrari Dino       Maserati Bora          Volvo 142E \n         -4.5431513          -2.7809399          -3.2053627          -1.0274952 \n\n\n \nIt is possible to explore the residuals checking them for some assumptions of a linear regression, and hidden patterns in the data. This is a very long topic, so it will be left to another post.\n\n\n2. Coefficients\nOverall, the coefficients contain information about the predictors of your dependent variable. Below you can see a detailed explanation.\n \n\n# Coefficients:\n#             Estimate Std. Error t value Pr(&gt;|t|)    \n# (Intercept)  37.2851     1.8776  19.858  &lt; 2e-16 ***\n# wt           -5.3445     0.5591  -9.559 1.29e-10 ***\n\n \na. Intercept\nIntercept is the mean value of y when x is 0 in our model. In our example, how many miles a gallon would make in average for a car with zero lbs. Also represented as β0 in the linear regression equation:\ny = β0 + β1x + ε\nIn our example it makes no sense at all. There is no car with 0 lbs. Therefore we can just ignore it and interpret the coefficient of interest, β1.\nb. wt\nRepresented by the variable name, this is the angular coefficient, β1. It represents the slope of the line in the linear regression plot. This is how much y increases or decreases with the increase of one unit of x. In our example, it is how many miles per gallon decreases (estimate is negative = -5.3) with the increase of each 1000 lb weight (unit of measure of wt) of the car.\nThe columns for the coefficients are:\nI. Estimate\nEstimates are the values per se of β0 (Intercept) and β1 (wt). The explanation of them is above.\nII. Std Error\nStandard Error is the deviation of the estimate from its real value. The smallest this value is, more precise the estimate is. It tends to be smaller if you have a big number of observations, since it is the standard deviation divided by n. \nIII. t value\nThis is the value of t in the Student’s t test for one sample. The alternative hypothesis tested is: is your estimate different than zero?, which relies on the distribution of your data being a t distribution, which is very close to a normal distribution. It is calculated by Estimate/Std Error.\nIV. Pr(&gt;|t|)\nThis is the p value for the t test applied to your estimate. If it is less than the considered alpha - the error you choose to accept (generally 0.05) - then you can say your estimates are very likely to be significant.\n\n\n3. Signif. codes\nThis is just a legend for what the asterisks and dot mean concerning level of significance for the p values.\n\n\n4. Degrees of freedom\n\n# 30 degrees of freedom\n\n \nDegrees of freedom (DF) are how many units of your data are “free to vary”.\nPractically, this is how many observations you have minus the number of estimated parameters used in your model (in this case, intercept and weight). Which means that it is directly proportional to the number of samples, and inversely proportional to the number of parameters and variables in your analysis. Degrees of freedom are crucial to determine your t distribution shape and what will be the significance (p value) of the estimates.\n\n\n5. Residual standard error\n \n\n# Residual standard error: 3.046\n\n \nResidual standard error (RSE) is the average deviation of predicted values (from the model) from observed values (the ones in your dataset).\nIt can be calculated with:\n\ny &lt;- mtcars$mpg\ny_hat &lt;- predict(fit)\ndf &lt;- 30 # degrees of freedom\n\nsqrt(sum((y - y_hat)^2)/df)\n\n[1] 3.045882\n\n\n \nStatistically speaking, it is the estimate of standard deviation of predicted values from real values; a measure of variation around the regression line.\nConfidence intervals around predicted values are generated using the RSE, therefore it is an important metric of your model. Large RSE can generate inaccurate predictions.\n\n\n6. Multiple R-squared\n \n\n# Multiple R-squared:  0.7528\n\n \nThis is the famous R squared (R²). The proportion of variance of your dependent variable, y, which is explained by your model. Simplifying: proportion of variance explained.\nSince it is a proportion, its value is between 0 and 1.\nOur model explains 75% of variance in miles per gallon.\nThis does not mean that if you have an R² = 0.3 for example you have a poor regression analysis. Some variables are just partly explained by the predictor analyzed, and if your intention is to interpret the influence of a predictor in a response variable, it may be useful even with a low variance explained. This happens in genomics, with polygenic scores for example, when your analyzed genetics explains only a part of a trait or disease, while the rest is explained by other factors such as environment.\nAlso, this is not the best way to analyze if your model is good or not. RMSE (Root Mean Squared Error), which is just squaring residuals, averaging them, and getting the square root, is generally a better way to evaluate whether you have a less prone to errors model. The smallest the RMSE, the better your model is.\n \n\nRMSE &lt;- sqrt(mean(resid(fit)^2))\n\nRMSE\n\n[1] 2.949163\n\n\n \nThat means that in average there is an error of 2.9 miles per gallon in our model.\nHave in mind what my boss always repeats: “All models are wrong, but some are useful”, a classical phrase by George Box.\n\n\n7. Adjusted R-Squared\n \n\n# Adjusted R-squared:  0.7446\n\n \nWhen there is more than one predictor in your linear regression (multiple linear regression), there is always an increase in R² independent of increase in variance explained, due to just the addittion of a new predictor. Therefore the R² value is adjusted for that.\nTo compare multiple linear regression models, or models with different number of predictors, it is recommended to check adjusted R-squared instead of multiple R-squared for variance explained.\n\n\n8. F-statistic\n \n\n# F-statistic: 91.38 on 1 and 30 DF\n\n \nF is the statistic from the F-test performed in your model. Basically the F value is used to see if there is any relationship between the response and predictors in a multiple linear regression. In the example discussed here, a simple linear regression, the estimate β1 = 0 is a better way to tell that there is no relationship between the response and predictor.\n\n\n9. p-value\n \n\n# p-value: 1.294e-10\n\n \nThis is the F-test p-value, which tells you if your F-test is significant. A p-value below the alpha you choose (for example, 0.05) means there is a high chance that at least one of your predictors is significantly associated with your dependent variable. It is more used for multiple regression models, which can be approached in another blog post."
  },
  {
    "objectID": "posts/linear-regression-coefs/index.html#interpretation-concerns",
    "href": "posts/linear-regression-coefs/index.html#interpretation-concerns",
    "title": "Understand Linear Regression Results in R",
    "section": "Interpretation concerns",
    "text": "Interpretation concerns\nSome transformations can be used in the linear regression to make your model more interpretable.\nFor example, let’s say that, as me, you are not from a country that uses lbs as a weight metric, neither miles for a distance one, or gallons for a volume one.\nInstead, you would like to interpret your model using kilograms, kilometers, and liters. Is this possible?\nYes!\nUsing the I() operator around the terms of the linear regression, just so you can transform from inside of the lm() function, we can transform the terms and get the proper interpretation. See below:\n \n\nmiles_to_kilometers &lt;- 1.61\ngallons_to_liters &lt;- 3.79\nlb_to_kg &lt;- 0.45\n\nfit_not_english &lt;- lm(I(mpg*miles_to_kilometers/gallons_to_liters) ~ I(wt*lb_to_kg), mtcars)\n\nsummary(fit_not_english)\n\n\nCall:\nlm(formula = I(mpg * miles_to_kilometers/gallons_to_liters) ~ \n    I(wt * lb_to_kg), data = mtcars)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.92994 -1.00453 -0.05318  0.59878  2.91954 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       15.8388     0.7976  19.858  &lt; 2e-16 ***\nI(wt * lb_to_kg)  -5.0452     0.5278  -9.559 1.29e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.294 on 30 degrees of freedom\nMultiple R-squared:  0.7528,    Adjusted R-squared:  0.7446 \nF-statistic: 91.38 on 1 and 30 DF,  p-value: 1.294e-10\n\n\n \nThe interpretation is:\n\nThere is a reduction of 5 kilometers per liter for each 1000kg increase in the car weight.\n\nMuch better now, right?\n;)"
  },
  {
    "objectID": "posts/linear-regression-coefs/index.html#closing-remarks",
    "href": "posts/linear-regression-coefs/index.html#closing-remarks",
    "title": "Understand Linear Regression Results in R",
    "section": "Closing remarks",
    "text": "Closing remarks\nI hope you have liked this post, and that it has shed light on you about how to interpret the regression models outputs from R.\nI really missed this content when I was applying linear regression in R, so I really hope it helps you as much as it would have helped me when I was looking for it!"
  },
  {
    "objectID": "posts/linear-regression-coefs/index.html#read-more",
    "href": "posts/linear-regression-coefs/index.html#read-more",
    "title": "Understand Linear Regression Results in R",
    "section": "Read more",
    "text": "Read more\n\nChapter 3.2 of the book An Introduction to Statistical Learning (Second Edition) - you can pick the R or Python version to download for free.\nRegression Models Coursera Course from Johns Hopkins free GitHub content."
  },
  {
    "objectID": "posts/big-data-bioinformatics/index.html",
    "href": "posts/big-data-bioinformatics/index.html",
    "title": "How To Operate Large Files in Bioinformatics",
    "section": "",
    "text": "Situation: you start your bioinformatics project very happy that you will be analyzing your sequencing data, be it transcriptomic, genomic or epigenomic; you have learned bash, R and statistics, and then you try to transfer all of the data to your computer, and then you see that, bleh, you don’t have enough storage on your computer. Then what?\nYou start panicking? No!\nYou start searching the web for any information on that, and come up with a gazillion new terms that you have never heard before. Server, cluster, cloud computing, ssh, ssd, bucket, to enumerate a few. And now you panic for real.\n.\nDon’t worry, I am here to help! I’ll get straight to the point about the options available to tackle this problem when working with big data in bioinformatics.\nBasically you have two main options:"
  },
  {
    "objectID": "posts/big-data-bioinformatics/index.html#cloud-computing",
    "href": "posts/big-data-bioinformatics/index.html#cloud-computing",
    "title": "How To Operate Large Files in Bioinformatics",
    "section": "Cloud computing",
    "text": "Cloud computing\nThese are the services provided by companies in which you can pay for a temporary remote machine to run your analyses. Google Cloud, Amazon AWS and Azure are the most well-known providers of this type of service. Basically you need to create an account, link to a payment method, and start using it.\nIf this is your option, you need to learn the basics of analyzing data in the cloud. I would go with the simplest way, which is creating an instance. An instance is nothing more than a computer you configure remotely. You need to pick a size of CPU, RAM memory and storage, which can be HDD (popularly known as HD, cheaper and slower) or SSD (faster to transfer data, but more expensive). Then, you just transfer your data to the instance using wget or ssh, depending on where this data will come from, and you can work from there (using the command line, of course).\n\nGoogle Cloud Compute Engine\nBelow I will give you some details on how to do this on Google Cloud Compute Engine. They give $300 of free credits to new Google Cloud users, so you can test and potentially do the analysis you need for free. To give you an idea of costs, a general use machine with 4 CPUs, 15GB of memory (RAM) and 100GB of HDD storage, running for 30 days would cost you 140 USD. You can calculate the expected amount for your personalized needs in the calculator page.\nAfter setting up your instance and transferring the data, you need to install all software needed to run your analyses from scratch. For example, let’s say that you want to run an RNA-seq alignment using STAR. You will need to install STAR, transfer the reference genome or transcriptome to the instance, build the reference for this program, and then run it. Don’t forget that any other tools you might need for previous steps such as QC must be installed to the instance too. After all, it is a new computer you are renting for a limited time.\nAfter finishing your analysis, you need to deal with the resulting data. Let’s talk about this in another post, because it is a topic that deserves better attention."
  },
  {
    "objectID": "posts/big-data-bioinformatics/index.html#servercluster-in-your-university",
    "href": "posts/big-data-bioinformatics/index.html#servercluster-in-your-university",
    "title": "How To Operate Large Files in Bioinformatics",
    "section": "Server/cluster in your university",
    "text": "Server/cluster in your university\n\n\n\nPhoto by Taylor Vick on Unsplash\n\n\nIn this option there are two different ways that a server might be provided to you: via a job schedule strategy or by giving you access to a slice of this server.\n\nJob schedule strategy\nA server, as the instance in the cloud computing resource, is a remote computer to which you have access. The job schedule strategy requires you to learn how to structure your data and then you need to write your scripts and submit them to the job scheduler, then wait for your turn to run them, as other people send their jobs to the scheduler too. The most popular examples are Slurm and SGE.\nGenerally there is a free limited space (scratch) of the server for you to experiment with a slice of the data (so you are certain that your script is correct to send to schedule for example). Some more complex labs allow you to send to a specific server schedule line, depending on your demand of CPU (or GPU) and memory. These are aspects you may require by task as well when submitting your job to the scheduler.\n\n\nUsing a slice of the server\nAnother way of doing this, generally in less experienced labs, is by requesting a specific slice of the server available in the university for you to use during your full project. That would be: “I need 100GB disk with 16GB of memory and 4 CPU to run my analysis”. In my PhD I knew that I could have asked for it in my university, but I did not have a bit of idea on what would be my CPU, RAM memory and storage needs. Therefore I could not get it. I still relied on my collaborator’s server to finish my analyses. So this is an important knowledge to have.\nCalculating the necessary computational resources for your bioinformatics analyses will need to be addressed in a new blog post.\nI hope this post has helped you on your bioinformatics journey!"
  }
]